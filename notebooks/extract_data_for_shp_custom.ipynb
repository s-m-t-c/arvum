{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5488c8e-1f09-4f53-b71d-0b249cff4c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ncpus = 31\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Notebook to extract training and validation data from shape files to a text file parallelised across features\n",
    "\n",
    "Inputs custom function for temporal statistics calculation or multiple products\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Load modules\n",
    "\n",
    "import os\n",
    "import datacube\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import subprocess as sp\n",
    "import geopandas as gpd\n",
    "from pathlib import Path\n",
    "from odc.io.cgroups import get_cpu_quota\n",
    "from datacube.utils.geometry import assign_crs\n",
    "\n",
    "import sys\n",
    "from dea_tools.bandindices import calculate_indices\n",
    "from dea_tools.classification import collect_training_data\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Need ls5 for 2010 and ls8 for 2015+\n",
    "time = \"2010\"\n",
    "product = [\"ga_ls5t_nbart_gm_cyear_3\"]\n",
    "# ga_ls8c_nbart_gm_cyear_3\n",
    "\n",
    "# Point the path to the correct file - this can be either the training data or the validation data\n",
    "# Example of path for validation: /home/jovyan/arvum/data/dea_landcover/validation/{time}_validation_complete.shp\n",
    "\n",
    "path = f\"/home/jovyan/arvum/data/dea_landcover/{time}_merged/{time}_merged.shp\"\n",
    "\n",
    "# The field variable refers to the true classification column.\n",
    "# The true classification column for the training data is classnum \n",
    "# while the tre classification in the validation data is called output\n",
    "\n",
    "field = \"classnum\"\n",
    "#field = 'output'\n",
    "\n",
    "zonal_stats = 'median'\n",
    "resolution = (-30, 30)\n",
    "fail_ratio = 0.05\n",
    "fail_threshold  = 0.02\n",
    "return_coords=True\n",
    "\n",
    "ncpus = round(get_cpu_quota())\n",
    "print('ncpus = ' + str(ncpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "281c73e6-17d5-471c-ac55-b0b9af17725d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_layers(query):\n",
    "    \n",
    "    # Connect to the datacube\n",
    "    dc = datacube.Datacube(app='custom_feature_layers')\n",
    "    \n",
    "    # Load ls geomedian\n",
    "    ds = dc.load(product=product, **query, measurements=['blue', 'green', 'red', 'nir', 'swir1', 'swir2', 'sdev', 'edev', 'bcdev'])       \n",
    "    # Calculate some band indices    \n",
    "    gm = calculate_indices(ds, index=[\"NDVI\", \"MNDWI\", \"BAI\", \"BUI\", \"BSI\", \"TCG\", \"TCW\", \"TCB\", \"NDMI\", \"LAI\", \"EVI\", \"AWEI_sh\", \"BAEI\", \"NDSI\", \"SAVI\", \"NBR\"], drop=False, collection=\"ga_ls_3\")\n",
    "    fc = dc.load(product='ga_ls_fc_pc_cyear_3', time=time, like=ds.geobox)\n",
    "  \n",
    "    output = xr.merge([gm, fc])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1ae034b-d3c8-4cae-ac11-1d4cc7139d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = {\n",
    "    \"time\": time,\n",
    "    \"resolution\": resolution,\n",
    "    \"group_by\": \"solar_day\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26a88610-415b-40f2-8191-8db65f84909d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is only used for generating the validation data\n",
    "# given the true classification is stored in th  \n",
    "\n",
    "input_data = gpd.read_file(path)\n",
    "\n",
    "# For the validation data convert the output to numerical\n",
    "# input_data['output'] = pd.to_numeric(input_data['output'])\n",
    "\n",
    "# Plot first five rows\n",
    "# input_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2cb96275-d86b-41cd-a08b-d6beb37c7596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taking zonal statistic: median\n",
      "Collecting training data in parallel mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36491466728e45a7a75b0085f09eaefc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of possible fails after run 1 = 0.0 %\n",
      "Removed 0 rows wth NaNs &/or Infs\n",
      "Output shape:  (100, 36)\n",
      "CPU times: user 277 ms, sys: 198 ms, total: 475 ms\n",
      "Wall time: 15.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "column_names, model_input = collect_training_data(\n",
    "    gdf=input_data[0:100],\n",
    "    dc_query=query,\n",
    "    ncpus=ncpus,\n",
    "    return_coords=False,\n",
    "    field=field,\n",
    "    zonal_stats=zonal_stats,\n",
    "    feature_func=feature_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5bcb742-c027-44c2-a047-af2df8e9ca6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['output', 'blue', 'green', 'red', 'nir', 'swir1', 'swir2', 'sdev', 'edev', 'bcdev', 'NDVI', 'MNDWI', 'BAI', 'BUI', 'BSI', 'TCG', 'TCW', 'TCB', 'NDMI', 'LAI', 'EVI', 'AWEI_sh', 'BAEI', 'NDSI', 'SAVI', 'NBR', 'pv_pc_10', 'pv_pc_50', 'pv_pc_90', 'bs_pc_10', 'bs_pc_50', 'bs_pc_90', 'npv_pc_10', 'npv_pc_50', 'npv_pc_90', 'qa']\n",
      "\n",
      "[[ 112.  471.  947. ...   33.   47.    2.]\n",
      " [ 112.  551. 1144. ...   50.   54.    2.]\n",
      " [ 112.  536. 1124. ...   27.   40.    2.]\n",
      " ...\n",
      " [ 112.  558.  937. ...   39.   56.    2.]\n",
      " [ 111.  769. 1130. ...   58.   84.    2.]\n",
      " [ 111.  809. 1396. ...   41.   62.    2.]]\n"
     ]
    }
   ],
   "source": [
    "# Inspect the data\n",
    "print(column_names)\n",
    "print()\n",
    "print(np.array_str(model_input, precision=2, suppress_small=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c9bd66f-f94d-458f-9aeb-e93da066c595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 37)\n"
     ]
    }
   ],
   "source": [
    "# Save the data to disk\n",
    "\n",
    "model_input = np.hstack((model_input, np.full((model_input.shape[0], 1), int(time))))\n",
    "print(model_input.shape)\n",
    "\n",
    "column_names.append(\"time\")\n",
    "\n",
    "# Change the name if you're working with the validation data\n",
    "output_file = f\"{time}_validation_data.csv\"\n",
    "\n",
    "# Add a binary classification column to the data and remove the multi-class variable \n",
    "data = pd.DataFrame(data=model_input, columns=column_names)\n",
    "\n",
    "# Comment the following two lines when running the validation set\n",
    "data['binary_class'] = data['classnum'].apply(lambda x: 1 if x==111 else 0)\n",
    "data.drop(labels=['classnum'], axis=1, inplace=True)\n",
    "data.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b22ffaa-d9fa-4ec8-8361-02dff85489f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this snippet to combine 2010 and 2015 training data\n",
    "# This file will be used to train the cultivated model\n",
    "\n",
    "data_2010 = pd.read_csv(\"/home/jovyan/arvum/data/dea_landcover/c3/2010_training_data.csv\")\n",
    "data_2015 = pd.read_csv(\"/home/jovyan/arvum/data/dea_landcover/c3/2015_training_data.csv\")\n",
    "\n",
    "data = pd.concat([data_2010, data_2015])\n",
    "data.to_csv(\"/home/jovyan/arvum/data/dea_landcover/c3/2010_2015_training_data.csv\",  index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
