{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test application of ML classifier to data\n",
    "\n",
    "Takes pickled traing sklearn model and applies to data from DEA\n",
    "\n",
    "Model is trained using data extracted to a CSV file, can be downloaded from: https://rsg.pml.ac.uk/shared_files/dac/train_input_geomedian_tmad_subset.txt.gz\n",
    "\n",
    "The model is trained using something like the code below:\n",
    "\n",
    "```python\n",
    "import pickle\n",
    "import numpy\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Read in text file\n",
    "model_input = numpy.loadtxt(\"train_input_geomedian_tmad.txt\", skiprows=1)\n",
    "\n",
    "# Headers are\n",
    "# classnum blue green red nir swir1 swir2 BUI BSI NBI EVI NDWI MSAVI sdev edev bcdev\n",
    "column_names = 'classnum blue green red nir swir1 swir2 BUI BSI NBI EVI NDWI MSAVI sdev edev bcdev'.split()\n",
    "\n",
    "# Set up model\n",
    "model = RandomForestClassifier(n_estimators=100, n_jobs=-1, max_depth=10, max_features=3, verbose=2, oob_score=True)\n",
    "\n",
    "# Train model\n",
    "classifier = model.fit(model_input[:,[3, 4, 5, 6, 13, 14, 15]], model_input[:,0])\n",
    "\n",
    "# Pickle model\n",
    "with open('model_pickle.pickle', 'wb') as f:\n",
    "    pickle.dump(classifier,f)\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "import datacube\n",
    "from datacube import helpers\n",
    "from datacube.utils import geometry\n",
    "from matplotlib import pyplot\n",
    "import numpy\n",
    "import sklearn\n",
    "import xarray\n",
    "import yaml\n",
    "\n",
    "\n",
    "# Load in modules from repos\n",
    "sys.path.append('/home/jovyan/development/LCCS/decision_tree')\n",
    "import dea_classificationtools\n",
    "\n",
    "sys.path.append('/home/jovyan/development/dea-notebooks/Scripts')\n",
    "from dea_bandindices import calculate_indices\n",
    "from dea_plotting import display_map\n",
    "\n",
    "sys.path.append('/home/jovyan/development/livingearth_lccs')\n",
    "from le_lccs.le_classification import lccs_l3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up working dir\n",
    "working_dir = '/home/jovyan/cultivated_classification'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in pickled data\n",
    "with open(os.path.join(working_dir, 'model_pickle.pickle'), 'rb') as f:\n",
    "    classifier = pickle.load(f)\n",
    "    \n",
    "model_variable_names = 'blue green red nir swir1 swir2 BUI BSI NBI EVI NDWI MSAVI sdev edev bcdev'.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_classification_for_site(site_name, model_variable_names=None):\n",
    "    \"\"\"\n",
    "    Function to run the classification for a given site.\n",
    "    \n",
    "    Gets bounds of site from yaml file\n",
    "    \"\"\"\n",
    "    # Specify site\n",
    "\n",
    "    # Read in config file with site bounds\n",
    "    with open(\"au_test_sites.yaml\", \"r\") as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    \n",
    "    # Get bounds\n",
    "    x = (config[site_name][\"min_x\"],config[site_name][\"max_x\"])\n",
    "    y = (config[site_name][\"max_y\"],config[site_name][\"min_y\"])\n",
    "\n",
    "    query = {'time': ('2015-01-01', '2015-02-01')}\n",
    "    query['x'] = x\n",
    "    query['y'] = y\n",
    "    query['crs'] = 'EPSG:3577'\n",
    "    query['resolution'] = (-100, 100)\n",
    "    \n",
    "    dc = datacube.Datacube(app = 'classifiers')\n",
    "\n",
    "    geomedian_data = dc.load(product='ls8_nbart_geomedian_annual', group_by='solar_day',\n",
    "                             dask_chunks={'x' : 1000, 'y' : 1000}, **query)\n",
    "    \n",
    "    \n",
    "    geomedian_data = calculate_indices(geomedian_data, 'BUI', collection='ga_ls_2')\n",
    "    geomedian_data = calculate_indices(geomedian_data, 'BSI', collection='ga_ls_2')\n",
    "    geomedian_data = calculate_indices(geomedian_data, 'NBI', collection='ga_ls_2')\n",
    "    geomedian_data = calculate_indices(geomedian_data, 'EVI', collection='ga_ls_2')\n",
    "    geomedian_data = calculate_indices(geomedian_data, 'NDWI', collection='ga_ls_2')\n",
    "    geomedian_data = calculate_indices(geomedian_data, 'MSAVI', collection='ga_ls_2')\n",
    "    \n",
    "    mads_data = dc.load(product='ls8_nbart_tmad_annual', group_by='solar_day',\n",
    "                        dask_chunks={'x' : 1000, 'y' : 1000}, **query)\n",
    "\n",
    "    # Join geomedian + mads\n",
    "    new_data = xarray.merge([geomedian_data, mads_data])\n",
    "    \n",
    "    # Subset to just use the variable names from the model\n",
    "    if model_variable_names is not None:\n",
    "        new_data = new_data[model_variable_names]\n",
    "        \n",
    "    predicted = dea_classificationtools.predict_xr(classifier, new_data, progress=True)\n",
    "    \n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop thorough all site and export a geotiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"au_test_sites.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, site_name in enumerate(config.keys()):\n",
    "    \n",
    "    print(\"[{:02}/{:02}] {}\".format(i+1, len(config.keys()),site_name))\n",
    "    \n",
    "    predicted = run_classification_for_site(site_name, model_variable_names=model_variable_names)\n",
    "    \n",
    "    # Get only cultivated layer\n",
    "    cultivated = predicted.where(predicted == 111)\n",
    "\n",
    "    out = cultivated.isel(time=0).transpose()\n",
    "    out = cultivated.to_dataset(name=\"cultivated\")\n",
    "    out.attrs['crs']=geometry.CRS(geomedian_data.crs)\n",
    "    out = out.isel(time=0)\n",
    "    \n",
    "    helpers.write_geotiff(os.path.join(working_dir, '{}_cultivated.tif'.format(site_name.lower().replace(' ','_'))),\n",
    "                          out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colour and plot the classified data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red, green, blue, alpha = lccs_l3.colour_lccs_level3(predicted[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.imshow(numpy.dstack([red, green, blue, alpha]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
